import { GoogleGenerativeAI } from '@google/generative-ai';

// Gemini API Key aus Umgebungsvariablen
const apiKey = import.meta.env.VITE_GEMINI_API_KEY || '';

if (!apiKey) {
    console.warn('‚ö†Ô∏è Gemini API Key nicht konfiguriert. F√ºge VITE_GEMINI_API_KEY zu deiner .env.local Datei hinzu.');
}

// Gemini Client initialisieren
const genAI = apiKey ? new GoogleGenerativeAI(apiKey) : null;

// Das Model (gemini-2.0-flash-exp ist das neueste kostenlose Modell)
const model = genAI?.getGenerativeModel({ model: 'gemini-2.0-flash-exp' });

// ============================================
// SYSTEM PROMPT - Die Pers√∂nlichkeit des Bots
// ============================================
export const SYSTEM_PROMPT = `System-Instruktionen: Nova (The Straight Line Analyst)

Deine Rolle:

Du bist Nova, ein Senior IT-Business Analyst. Du kombinierst technische Pr√§zision mit der Straight-Line-Persuasion-Psychologie. Dein Ziel ist nicht nur ein Lastenheft, sondern absolute Klarheit ("Certainty") beim Nutzer.

üî¥ KOMMUNIKATIONS-REGELN (STRIKT):

Kurz & Knackig: Max. 3-4 S√§tze.

Ein-Frage-Regel: Immer nur EINE Frage.

Bohren: Gib dich nicht mit Vagem zufrieden.

üê∫ THE STRAIGHT LINE LOGIC (Die 3x10 Regel):

Du musst sicherstellen, dass der Nutzer "on track" bleibt. Nutze daf√ºr diese psychologischen Checkpoints:

Die "Produkt-10" (Phase 0):

Bevor du ins Detail gehst, muss der Nutzer seine L√∂sung kaufen.

Technik: Wenn die L√∂sung steht (z.B. "Wir machen eine Web-App"), frage nach dem "Buy-In": "Macht das f√ºr dich Sinn, das so zu l√∂sen, oder hast du da noch Zweifel?"

Nur bei "Ja" weitergehen.

Die "Vertrauens-10" (W√§hrend Phase 1):

Der Nutzer muss merken, dass du ihn verstehst (Rapport).

Technik (Mirroring): Nutze seine Worte. Wenn er sagt "Das nervt mich", sagst du "Damit das nicht mehr nervt...".

Technik (Future Pacing): Wenn er genervt von Detailfragen ist, verkaufe ihm das Ergebnis: "Ich frage das so genau, damit die Entwickler sp√§ter nicht dein Geld verbrennen. Ist das okay f√ºr dich?" (Ein "Tie-Down").

Die "Prozess-10" (Vor Phase 2):

Bevor du das Lastenheft schreibst, hol dir das finale "Go".

Technik: "Ich glaube, ich habe jetzt einen Plan, der dein Problem endg√ºltig l√∂st. Bereit f√ºr das Ergebnis?"

DER PROZESS:

Phase 0: Die Diagnose & Die L√∂sung

Finde das Problem. Schlage die technische L√∂sung vor.

Wolf-Regel: Hol dir die Best√§tigung (Die "Produkt-10"), dass diese L√∂sung genau das ist, was er will.

Phase 1: Das Tiefen-Interview (Looping)

Arbeite die Sektoren A-D ab.

Looping: Wenn der Nutzer ausweicht oder vage ist ("Keine Ahnung, mach einfach"), akzeptiere das nicht. "Loope" zur√ºck zum Schmerzpunkt: "Wenn wir das hier offen lassen, wird die App sp√§ter genau dort Fehler machen. Lass uns das kurz kl√§ren: [Frage wiederholen]?"

Sektor A: Der Context (Wer, Wo, Warum?)Sektor B: Die Funktionen (Input -> Logik -> Output. Frage nach Edge Cases!)Sektor C: Design & Feeling (Dark Mode? Seri√∂s oder spielerisch?)Sektor D: Technik (Plattform, Daten, Sicherheit)

Phase 2: Die Erstellung (Developer-Ready)

Erstelle das Lastenheft strikt f√ºr Entwickler (Technisch, Bulletpoints).

1. Management Summary (Business Case)2. User Flow & Personas (Wer macht was?)3. Funktionale Specs (Features, Logik, Edge Cases)4. Tech Stack & Non-Functionals (Performance, Security, APIs)5. Datenmodell (Entit√§ten)

START-ANWEISUNG:

Begr√º√üe den Nutzer als Nova. Frage direkt und offen: "Hi! Erz√§hl mir, was in deinem Business gerade Kopfschmerzen bereitet ‚Äì oder hast du schon eine Idee, die wir umsetzen sollen?"`;

// ============================================
// CHAT-FUNKTION
// ============================================

export interface ChatMessage {
    role: 'user' | 'model';
    parts: { text: string }[];
}

let chatHistory: ChatMessage[] = [];

/**
 * Sendet eine Nachricht an Gemini und erh√§lt eine Antwort
 */
export async function sendMessage(userMessage: string): Promise<string> {
    if (!model) {
        return 'Gemini ist nicht konfiguriert. Bitte f√ºge VITE_GEMINI_API_KEY zu deiner .env.local Datei hinzu.';
    }

    try {
        // Chat starten mit History
        const chat = model.startChat({
            history: [
                {
                    role: 'user',
                    parts: [{ text: 'System: ' + SYSTEM_PROMPT }]
                },
                {
                    role: 'model',
                    parts: [{ text: 'Verstanden! Ich bin bereit, das Gespr√§ch zu f√ºhren.' }]
                },
                ...chatHistory
            ],
            generationConfig: {
                maxOutputTokens: 500,
                temperature: 0.7, // Etwas kreativ, aber nicht zu wild
            }
        });

        // Nachricht senden
        const result = await chat.sendMessage(userMessage);
        const response = result.response.text();

        // History aktualisieren
        chatHistory.push(
            { role: 'user', parts: [{ text: userMessage }] },
            { role: 'model', parts: [{ text: response }] }
        );

        return response;

    } catch (error) {
        console.error('Gemini Fehler:', error);
        return 'Entschuldigung, da ist etwas schiefgelaufen. Kannst du das nochmal sagen?';
    }
}

/**
 * Setzt die Chat-History zur√ºck (f√ºr neue Gespr√§che)
 */
export function resetChat(): void {
    chatHistory = [];
}

/**
 * Gibt die aktuelle Chat-History zur√ºck
 */
export function getChatHistory(): ChatMessage[] {
    return chatHistory;
}

/**
 * Generiert die initiale Begr√º√üungsnachricht
 */
export async function getInitialMessage(): Promise<string> {
    if (!model) {
        return 'Hey! üëã Was hat dich diese Woche am meisten genervt?';
    }

    try {
        const chat = model.startChat({
            history: [
                {
                    role: 'user',
                    parts: [{ text: 'System: ' + SYSTEM_PROMPT }]
                },
                {
                    role: 'model',
                    parts: [{ text: 'Verstanden!' }]
                }
            ]
        });

        const result = await chat.sendMessage('Starte jetzt das Gespr√§ch mit einer kurzen, lockeren Begr√º√üung.');
        const response = result.response.text();

        // Die erste Bot-Nachricht zur History hinzuf√ºgen
        chatHistory.push({ role: 'model', parts: [{ text: response }] });

        return response;

    } catch (error) {
        console.error('Gemini Initialisierungsfehler:', error);
        return 'Hey! üëã Was hat dich diese Woche am meisten genervt?';
    }
}

export default { sendMessage, resetChat, getChatHistory, getInitialMessage };
